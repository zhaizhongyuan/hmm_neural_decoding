{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5918355f-c759-41e8-9cc9-64baf78695b3"
    }
   },
   "source": [
    "# HMM State Clustering\n",
    "In this notebook we'll explore a post-hoc method for clustering HMM states.\n",
    "The idea is, given an HMM which has been fit to data, we reduce the number of states hierarchically by merging pairs of states. Let's say we start with an HMM with K states. The idea is that we'll try merging every pair of states and see which merge makes the log-likelihood of the data go down the least. Once we find that pair, we have K-1 states, and we repeat this process until we have satisfactorily few states.\n",
    "\n",
    "**Note**: This notebook is a little rough around the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "nbpresent": {
     "id": "346a61a3-9216-480d-b5b8-39a78782a8c3"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "npr.seed(0)\n",
    "\n",
    "from sklearn.utils import check_array, check_random_state\n",
    "\n",
    "import ssm\n",
    "from ssm.util import find_permutation\n",
    "from ssm.plots import gradient_cmap, white_to_color_cmap\n",
    "\n",
    "import hmmlearn\n",
    "from hmmlearn import hmm\n",
    "from hmmlearn import _utils\n",
    "from hmmlearn.utils import normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "color_names = [\n",
    "    \"windows blue\",\n",
    "    \"red\",\n",
    "    \"amber\",\n",
    "    \"faded green\",\n",
    "    \"dusty purple\",\n",
    "    \"orange\"\n",
    "    ]\n",
    "\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "cmap = gradient_cmap(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike times are binned into shape for hmm: (6000, 90)\n"
     ]
    }
   ],
   "source": [
    "filename = '/Users/Caravaggio/Downloads/m1_fr0.1hz_30hz_0s_7200s_0.1s_bins_nooverlap.sav'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    spike_times_all_neurons = joblib.load(file)\n",
    "\n",
    "# Convert to numpy array\n",
    "spike_times_all_neurons = np.array(spike_times_all_neurons)\n",
    "\n",
    "# Transpose into (71999, 90)\n",
    "spike_times_all_neurons = spike_times_all_neurons.T\n",
    "\n",
    "# Truncate two hour recording to 10 minutes into (6000, 90), \n",
    "# 0th to 45th L5/6, 46th to 89th L2/3, 89th being the most shallow\n",
    "spike_times_all_neurons = spike_times_all_neurons[:6000]\n",
    "\n",
    "# Z-score each neuron, i.e. firing rates of each neuron across time sum to 0\n",
    "spike_times_all_neurons_zscored = stats.zscore(spike_times_all_neurons) # Default axis is 0\n",
    "\n",
    "data = spike_times_all_neurons_zscored\n",
    "print(f'Spike times are binned into shape for hmm: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster states from the original HMM\n",
    "\n",
    "The `merge_two_states` function below takes in a trained HMM, and indices of two states, s1 and s2. It outputs a new HMM where all states except for s1 and s2 are the same, along with the log-likelihood of the data under the new model.\n",
    "\n",
    "Here's how we merge two states: In the E-step of the EM algorithm, we obtain a T x K table, which has the probability of being in state K at time T for every time point. To merge state k1 and k2, we take the two columns of the table corresponding to these two states and sum them. From this, we get a new table which is K-1 x T. We then run an M-step as normal to get the best parameters for our new K-1 state model, and evaluate the log likelihood.\n",
    "\n",
    "**NOTE**: as written, the below function does not support inputs or masks, and it is limited to HMMs with stationary transitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fe5eaaf9ae492ca6419ddc6029abc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3165.9517860412598 seconds taken to fit a 100-state hmm model using ssm.\n",
      "87.18822884559631 seconds taken to fit a 100-state hmm model using hmmlearn.\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters\n",
    "time_bins = data.shape[0]   # number of time bins\n",
    "num_states = 100   # number of discrete states\n",
    "obs_dim = data.shape[1]       # dimensionality of observation\n",
    "\n",
    "# ssm_hmm\n",
    "ssm_hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n",
    "t_start = time.time()\n",
    "ssm_hmm.fit(data)\n",
    "t_end = time.time()\n",
    "print(f'{t_end - t_start} seconds taken to fit a 100-state hmm model using ssm.')\n",
    "\n",
    "# hmmlearn_hmm\n",
    "hmmlearn_hmm = hmmlearn.hmm.GaussianHMM(n_components=num_states, covariance_type=\"full\", n_iter=100, verbose=False)\n",
    "t_start = time.time()\n",
    "hmmlearn_hmm.fit(data)\n",
    "t_end = time.time()\n",
    "print(f'{t_end - t_start} seconds taken to fit a 100-state hmm model using hmmlearn.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datas be a list\n",
    "def merge_two_states_ssm(old_hmm, s1, s2, datas, observations=\"gaussian\"):\n",
    "    \n",
    "    def collapse_and_sum_2d(arr, i, j, axis=0):\n",
    "        assert axis <= 1\n",
    "        out = arr.copy()\n",
    "        if axis == 0:\n",
    "            out[i,:] += out[j,:]\n",
    "            return np.delete(out, j, axis=0)\n",
    "        if axis == 1:\n",
    "            out[:, i] += out[:, j]\n",
    "            return np.delete(out, j, axis=1)\n",
    "        \n",
    "    K = old_hmm.K\n",
    "    D = old_hmm.D\n",
    "    assert K >= 2\n",
    "    assert s1 < K\n",
    "    assert s2 < K\n",
    "    assert s1 != s2\n",
    "    datas = datas if isinstance(datas, list) else [datas]\n",
    "    inputs, masks, tags = [None], [None], [None]\n",
    "    expectations = [old_hmm.expected_states(data, input, mask, tag)\n",
    "                            for data, input, mask, tag in zip(datas, inputs, masks, tags)]\n",
    "    \n",
    "    # Merge expectations for 2 states\n",
    "    expectations_new = []\n",
    "    for (Ez, Ezz, py) in expectations:\n",
    "        T_curr = Ez.shape[0]\n",
    "        \n",
    "        # Put merged expectations in first column\n",
    "        Ez_new = collapse_and_sum_2d(Ez, s1, s2, axis=1)\n",
    "        \n",
    "        # Now merge Ezz\n",
    "        # Ezz will have shape 1, K, K\n",
    "        # so we get rid of the first dimension then add it back.\n",
    "        Ezz_new = collapse_and_sum_2d(Ezz[0], s1, s2, axis=0)\n",
    "        Ezz_new = collapse_and_sum_2d(Ezz_new, s1, s2, axis=1)\n",
    "        Ezz_new = Ezz_new[None, :, :]\n",
    "        \n",
    "        expectations_new.append((Ez_new, Ezz_new, py))\n",
    "    \n",
    "    # Perform M-Step to get params for new hmm\n",
    "    new_hmm = ssm.HMM(K-1, D, observations=observations)\n",
    "    new_hmm.init_state_distn.m_step(expectations_new, datas, inputs, masks, tags)\n",
    "    new_hmm.transitions.m_step(expectations_new, datas, inputs, masks, tags)\n",
    "    new_hmm.observations.m_step(expectations_new, datas, inputs, masks, tags)\n",
    "    \n",
    "    # Evaluate log_likelihood\n",
    "    expectations = [new_hmm.expected_states(data, input, mask, tag)\n",
    "                    for data, input, mask, tag in zip(datas, inputs, masks, tags)]\n",
    "    new_ll = new_hmm.log_prior() + sum([ll for (_, _, ll) in expectations])\n",
    "    return new_ll, new_hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_states_hmmlearn(hmm_old, s1, s2, data, observations=\"gaussian\"):\n",
    "    \n",
    "    def collapse_and_sum_1d(arr, i, j):\n",
    "        out = arr.copy()\n",
    "        out[i] += out[j]\n",
    "        return np.delete(out, j)\n",
    "    \n",
    "    def collapse_and_sum_2d(arr, i, j, axis=0):\n",
    "        assert axis <= 1\n",
    "        out = arr.copy()\n",
    "        if axis == 0:\n",
    "            out[i,:] += out[j,:]\n",
    "            return np.delete(out, j, axis=0)\n",
    "        if axis == 1:\n",
    "            out[:, i] += out[:, j]\n",
    "            return np.delete(out, j, axis=1)\n",
    "    \n",
    "    data = check_array(data)\n",
    "    hmm_old._check()\n",
    "   \n",
    "    start_prob = hmm_old.startprob_\n",
    "    start_prob_new = collapse_and_sum_1d(start_prob, s1, s2)\n",
    "    \n",
    "    transmtx = hmm_old.transmat_\n",
    "    transmtx_new = collapse_and_sum_2d(transmtx, s1, s2, axis=1)\n",
    "    transmtx_new = collapse_and_sum_2d(transmtx_new, s1, s2, axis=0)\n",
    "    \n",
    "    K = hmm_old.n_components\n",
    "    hmm_new = hmm.GaussianHMM(n_components=K-1, covariance_type=\"full\", verbose=True)\n",
    "    hmm_new._init(data)\n",
    "    hmm_new._check()\n",
    "    stats = hmm_new._initialize_sufficient_statistics()\n",
    "    stats['start'] = start_prob_new\n",
    "    stats['trans'] = transmtx_new\n",
    "    \n",
    "    lattice, log_prob, posteriors, fwdlattice, bwdlattice = \\\n",
    "                    hmm_old._fit_log(data)\n",
    "    posteriors_new = collapse_and_sum_2d(posteriors, s1, s2, axis=1)\n",
    "    post_new = posteriors_new.sum(axis=0)\n",
    "    stats['post'] = post_new\n",
    "    stats['obs'] = np.dot(posteriors_new.T, data)\n",
    "    stats['obs*obs.T'] += np.einsum('ij,ik,il->jkl', posteriors_new, data, data)\n",
    "    hmm_new._do_mstep(stats)\n",
    "    ll_new = hmm_new.score(data)\n",
    "    \n",
    "    return ll_new, hmm_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous log-likelihood is 1086513.2258466305\n",
      "current log-likelihood is 1065590.255249912\n",
      "likelihood drop:  20922.970596718602\n"
     ]
    }
   ],
   "source": [
    "# merge two states using hmmlearn\n",
    "hmm_old = hmmlearn_hmm\n",
    "ll_old = hmm_old.score(data)\n",
    "print('previous log-likelihood is ' + str(ll_old))\n",
    "ll_new, hmm_new = merge_two_states_hmmlearn(hmm_old, 0, 3, data)\n",
    "print('current log-likelihood is ' + str(ll_new))\n",
    "print(\"likelihood drop: \", ll_old - ll_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous log-likelihood is 1060408.579972545\n",
      "current log-likelihood is 1031250.3286563596\n",
      "likelihood drop:  29158.251316185342\n"
     ]
    }
   ],
   "source": [
    "# merge two states using ssm\n",
    "hmm_old = ssm_hmm\n",
    "ll_old = hmm_old.log_probability(data)\n",
    "print('previous log-likelihood is ' + str(ll_old))\n",
    "ll_new, hmm_new = merge_two_states_ssm(hmm_old, 0, 3, data)\n",
    "print('current log-likelihood is ' + str(ll_new))\n",
    "print(\"likelihood drop: \", ll_old - ll_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a pairwise similarity matrix\n",
    "\n",
    "We can use the log-likelihood drop when merging states as a proxy for state \"similarity.\" Two states which can be merged with minimal drop in likelihood might be considered similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_similarity_matrix_hmmlearn(hmm_original, data):\n",
    "    num_states = hmm_original.n_components\n",
    "    init_ll = hmm_original.score(data)\n",
    "    similarity = np.nan * np.ones((num_states, num_states))\n",
    "    merged_hmms = np.empty((num_states, num_states), dtype=object)\n",
    "    for s1 in range(num_states-1):\n",
    "        for s2 in range(s1+1, num_states):\n",
    "            merged_ll, merged_hmm = merge_two_states_hmmlearn(hmm_original, s1, s2, data)\n",
    "            similarity[s1, s2] =  merged_ll - init_ll\n",
    "            merged_hmms[s1, s2] = merged_hmm\n",
    "        print(f'Finished state {s1}')\n",
    "            \n",
    "    return similarity, merged_hmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity, new_hmms = make_similarity_matrix_hmmlearn(hmmlearn_hmm, data)\n",
    "im = plt.imshow(similarity)\n",
    "plt.ylabel(\"state 1\")\n",
    "plt.xlabel(\"state 2\")\n",
    "plt.title(\"similarity\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering by iteratively merging states\n",
    "We start with a K state HMM, then merge possible pair of states k1 and k2. We can see which are the best two states to merge by checking the new log-likelihood. We then rinse and repeat for our new K-1 state HMM, tracking the log-likelihood as we go, until there is only 1 state left. After each merge, we can show the observation distribution and new similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_cluster(hmm_original, data):\n",
    "    num_states = hmm_original.n_components\n",
    "    linkage = [None]\n",
    "    likelihood_drops = [0]\n",
    "    hmms = [hmm_original]\n",
    "    \n",
    "    for i in range(num_states - 1):\n",
    "        similarity, merged_hmms = make_similarity_matrix_hmmlearn(hmms[-1], data)\n",
    "        \n",
    "        # Find the most similar states\n",
    "        s1, s2 = np.where(similarity == np.nanmax(similarity))\n",
    "        s1, s2 = s1[0], s2[0]\n",
    "        linkage.append((s1, s2))\n",
    "        likelihood_drops.append(similarity[s1, s2])\n",
    "        hmms.append(merged_hmms[s1, s2])\n",
    "        print(\"merging \", s1, \"and\", s2)\n",
    "    \n",
    "    return linkage, likelihood_drops, hmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage, likelihood_drops, hmms = hierarchical_cluster(hmmlearn_hmm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now plot the dendrogram using likelihood drop as similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dendrogram(num_states, linkage, likelihood_drops):\n",
    "    # plt.figure()\n",
    "    \n",
    "    def _plot_level(s1, s2, likelihood_drop, xs, offset):\n",
    "        new_offset = offset - likelihood_drop\n",
    "        for x in xs:\n",
    "            plt.plot([x, x], [offset, new_offset], '-k', lw=2)\n",
    "        plt.plot([xs[s1], xs[s2]], [new_offset, new_offset], '-k', lw=2)\n",
    "\n",
    "        new_xs = xs.copy()\n",
    "        new_xs[s1] = xs[s1] + (xs[s2] - xs[s1]) * npr.rand()\n",
    "        new_xs = np.concatenate([new_xs[:s2], new_xs[s2+1:]])\n",
    "        return new_xs, new_offset\n",
    "    \n",
    "    xs = np.arange(num_states, dtype=float)\n",
    "    offset = 0\n",
    "    for (s1, s2), drop in zip(linkage[1:], likelihood_drops[1:]):\n",
    "        xs, offset = _plot_level(s1, s2, drop, xs, offset)\n",
    "        \n",
    "    plt.xlabel(\"state\")\n",
    "    plt.ylabel(\"likelihood drop\")\n",
    "        \n",
    "dendrogram(hmmlearn_hmm.n_components, linkage, likelihood_drops)\n",
    "\n",
    "plt.savefig('hmm_#_states_hierarchical_clustering.png', dpi=300, bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('hidenseek')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c0823e86baae0f96ace674699db2c6d9fd7927a1b7ac1ee15c642961b7774c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
