{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import functools\n",
    "from tqdm import notebook\n",
    "\n",
    "sys.path.append('../../../libraries/') # Append the path within which the user-defined class is in\n",
    "from data_loader import data_loader\n",
    "from behavior_loader import bsoid_loader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'../../../processed_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 4-hour z-scored bandpower features of lpf using Welch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-scored bandpower features are binned into shape: (48, 14400)\n"
     ]
    }
   ],
   "source": [
    "zscore_feats = pickle.load(open(os.path.join(data_dir, 'welch_bp/welch_nonoverlap_zscore_bp_0.0_s_offset.np'), 'rb'))\n",
    "print(f'Z-scored bandpower features are binned into shape: {zscore_feats.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive repetitive windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_feats_naive = np.repeat(zscore_feats, 10, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best umap-hdbscan model that seperates lpf most clearly and predict on lpf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_f1_score(X, y, test_size=0.2, average='macro'):\n",
    "    sample_size = np.ceil(1 / test_size) * 4\n",
    "    targets, target_cnts = np.unique(y, return_counts=True)\n",
    "    X_train_arr = [] \n",
    "    X_test_arr = [] \n",
    "    y_train_arr = [] \n",
    "    y_test_arr = []\n",
    "    for i, target_cnt in enumerate(target_cnts):\n",
    "        if target_cnt >= sample_size:\n",
    "            target_idcs = np.where(y == targets[i])[0]\n",
    "            X_train_target, X_test_target, y_train_target, y_test_target = train_test_split(X[target_idcs, :], y[target_idcs], test_size=test_size, random_state=42)\n",
    "            X_train_arr.append(X_train_target)\n",
    "            X_test_arr.append(X_test_target)\n",
    "            y_train_arr.append(y_train_target)\n",
    "            y_test_arr.append(y_test_target)\n",
    "    X_train = np.vstack(X_train_arr)\n",
    "    X_test = np.vstack(X_test_arr)\n",
    "    y_train = np.hstack(y_train_arr)\n",
    "    y_test = np.hstack(y_test_arr)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced_subsample', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predict = clf.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, predict, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_hdbscan_rf_segment(data, n_components, n_neighbors, min_dist, random_state_umap, min_cluster_size, min_samples):\n",
    "    reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state_umap)\n",
    "    embedding = reducer.fit_transform(data)\n",
    "    clusterer_umap = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True).fit(embedding)\n",
    "    X = data[clusterer_umap.labels_ >= 0, :]\n",
    "    y = clusterer_umap.labels_[clusterer_umap.labels_ >= 0]\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced_subsample', n_jobs=-1)\n",
    "    clf.fit(X, y)\n",
    "    y_all = clf.predict(data)\n",
    "\n",
    "    return y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_states = umap_hdbscan_rf_segment(data=zscore_feats.T,\n",
    "                                     n_components=5,\n",
    "                                     n_neighbors=60,\n",
    "                                     min_dist=0,\n",
    "                                     random_state_umap=29,\n",
    "                                     min_cluster_size=150,\n",
    "                                     min_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([ 565, 1249, 4505, 1627,  601,  924,  882, 1353, 1674, 1020]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(lfp_states, return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive repetitive windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_predict_naive = np.repeat(lfp_states, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 4-hour l5 spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike times are binned into shape: (143999, 363)\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_dir, 'all_fr0.1hz_30hz_0s_14400s_0.1s_bins_nooverlap.npy'), 'rb') as file:\n",
    "    spike_times_all_neurons = np.load(file)\n",
    "\n",
    "# Convert to numpy array\n",
    "spike_times_all_neurons = np.array(spike_times_all_neurons)\n",
    "\n",
    "# # (Number of 100-ms bins, Number of l2 neurons)\n",
    "# T_1h, D_m1 = spike_times_all_neurons_1h.shape\n",
    "\n",
    "# Z-score each neuron, i.e. firing rates of each neuron across time sum to 0\n",
    "spike_times_all_neurons_4h_zscored = stats.zscore(spike_times_all_neurons) # Default axis is 0\n",
    "\n",
    "spikes = spike_times_all_neurons_4h_zscored\n",
    "print(f'Spike times are binned into shape: {spikes.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144000, 363)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes = np.vstack([spikes, np.random.rand(1, 363)])\n",
    "spikes.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 4-hour behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse stayed in nest for 0.28108749470484795% of time\n",
      "Smooth window: 6\n",
      "File #0 (a 6 body parts by 984391 frames) has 30 classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(144000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load behaviors\n",
    "behavior_loader = bsoid_loader(data_dir, 'ag25290_day2_iter13')\n",
    "f_index, filtered_data, smoothed_predictions = behavior_loader.main()\n",
    "\n",
    "# Correct prediction start\n",
    "framerate = 60\n",
    "delay = 6.8503\n",
    "behavior_start = int(delay * framerate)  # start of behavior\n",
    "smoothed_predictions_4h = smoothed_predictions[behavior_start:(behavior_start+(3600*60)*4):int(60/10)]\n",
    "smoothed_predictions_4h.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into 3 hour for train and 1 hour for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hours = 3\n",
    "\n",
    "bp_naive_train = np.hstack([zscore_feats_naive.T[:60*60*10*train_hours], ])\n",
    "bp_naive_test = zscore_feats_naive.T[60*60*10*train_hours:]\n",
    "\n",
    "lfp_predict_naive_train = lfp_predict_naive[:60*60*10*train_hours]\n",
    "lfp_predict_naive_test = lfp_predict_naive[60*60*10*train_hours:]\n",
    "\n",
    "spikes_train = spikes[:60*60*10*train_hours]\n",
    "spikes_test = spikes[60*60*10*train_hours:]\n",
    "\n",
    "smoothed_predictions_train = smoothed_predictions_4h[:60*60*10*train_hours]\n",
    "smoothed_predictions_test = smoothed_predictions_4h[60*60*10*train_hours:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 363)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_of_interest_0 = np.array([7, 8, 9, 10, 11, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 29]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_seg_prepare(data, targets, spikes, smoothed_predictions, random_state=10, limit_count=100, verbose=False):\n",
    "    X = data.copy()\n",
    "    y = targets.copy()\n",
    "    X_subsampled = []\n",
    "    y_subsampled = []\n",
    "    spikes_subsampled = []\n",
    "    smoothed_predictions_subsampled = []\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    # sig_behavs = [i for i, s in enumerate(behav_names_mod) if s != 'insignificant']\n",
    "    sig_behavs = behavior_of_interest_0\n",
    "    for sig_behav in sig_behavs:\n",
    "        idcs_s = np.where(y == sig_behav)[0]\n",
    "        idcs_s_all = np.where(smoothed_predictions_train == sig_behav)[0]\n",
    "        try:\n",
    "            sampled_idcs_s = rng.choice(len(idcs_s), limit_count, replace=False)\n",
    "            sampled_idcs_s_all = rng.choice(len(idcs_s_all), limit_count, replace=False)\n",
    "        except:\n",
    "            sampled_idcs_s = rng.choice(len(idcs_s), len(idcs_s), replace=False)\n",
    "            sampled_idcs_s_all = rng.choice(len(idcs_s_all), len(idcs_s), replace=False)\n",
    "        y_subsampled.append(y[idcs_s[sampled_idcs_s]])\n",
    "        X_subsampled.append(X[idcs_s[sampled_idcs_s], :])          \n",
    "        smoothed_predictions_subsampled.append(smoothed_predictions[idcs_s_all[sampled_idcs_s_all]])\n",
    "        spikes_subsampled.append(spikes[idcs_s_all[sampled_idcs_s_all], :])\n",
    "            \n",
    "    if verbose:\n",
    "        print(np.hstack(y_subsampled).shape, np.vstack(X_subsampled).shape)\n",
    "\n",
    "    return np.hstack(y_subsampled), np.vstack(X_subsampled), np.hstack(smoothed_predictions_subsampled), np.vstack(spikes_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_f1_score(X, y, test_size=0.2):\n",
    "    sample_size = np.ceil(1 / test_size) * 4\n",
    "    targets, target_cnts = np.unique(y, return_counts=True)\n",
    "    X_train_arr = [] \n",
    "    X_test_arr = [] \n",
    "    y_train_arr = [] \n",
    "    y_test_arr = []\n",
    "    for i, target_cnt in enumerate(target_cnts):\n",
    "        if target_cnt >= sample_size:\n",
    "            target_idcs = np.where(y == targets[i])[0]\n",
    "            X_train_target, X_test_target, y_train_target, y_test_target = train_test_split(X[target_idcs, :], y[target_idcs], test_size=test_size, random_state=42)\n",
    "            X_train_arr.append(X_train_target)\n",
    "            X_test_arr.append(X_test_target)\n",
    "            y_train_arr.append(y_train_target)\n",
    "            y_test_arr.append(y_test_target)\n",
    "    X_train = np.vstack(X_train_arr)\n",
    "    X_test = np.vstack(X_test_arr)\n",
    "    y_train = np.hstack(y_train_arr)\n",
    "    y_test = np.hstack(y_test_arr)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced_subsample', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predict = clf.predict(X_test)\n",
    "\n",
    "    return clf, np.unique(y_test), f1_score(y_test, predict, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rfs(rf_a, rf_b):\n",
    "    rf_a.estimators_ += rf_b.estimators_\n",
    "    rf_a.n_estimators = len(rf_a.estimators_)\n",
    "    return rf_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train state-aware classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_clf, _, _ = rf_f1_score(bp_naive_train, lfp_predict_naive_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acd22ec1d204b808b0404ed97590ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f225f432d77f417bafeff1f834cb2954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e2a79c17b84b4d992098afa5bb5685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7315e51b1540ae8112ee68ab155dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec2469d8e4a4a5a8e0dd239a4e50d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0afb690f4d2453a9952733730a6d1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd6c8f31cab429ca1d7ca7faf2d202c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163ba57425404ae095e9e4c8f089b629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b68830a1f2a4b66b5bd2899e535224e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddccef14db5248cbbc8d47e7b355c003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88ff5ba3f1d45f4a54aa14bbdafc263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_clf_dict = {}\n",
    "for lfp_state in notebook.tqdm(np.unique(lfp_predict_naive_train)):\n",
    "    state_clfs = []\n",
    "    for seed in notebook.tqdm(range(100, 201)):\n",
    "        seg_idcs = np.where(lfp_predict_naive_train == lfp_state)[0]\n",
    "        state_spikes = spikes_train[seg_idcs, :]\n",
    "        state_smoothed_predictions = smoothed_predictions_train[seg_idcs]\n",
    "        \n",
    "        state_smoothed_predictions_subsampled, state_spikes_subsampled, smoothed_predictions_subsampled, spikes_subsampled = \\\n",
    "            data_seg_prepare(state_spikes, state_smoothed_predictions, spikes_train, smoothed_predictions_train, random_state=seed)\n",
    "        \n",
    "        state_clf, _, _ = rf_f1_score(state_spikes_subsampled, state_smoothed_predictions_subsampled)\n",
    "        state_clfs.append(state_clf)\n",
    "    state_clf_dict[lfp_state] = functools.reduce(combine_rfs, state_clfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_predict = []\n",
    "for i, lfp_state in enumerate(lfp_clf.predict(bp_naive_test)):\n",
    "    behav_predict.append(state_clf_dict[lfp_state].predict(np.reshape(spikes_test[i, :], (1, 363))))\n",
    "f1_score(behav_predict, smoothed_predictions_test, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b4524c2285503ee5371e1693aa576532508aa4de32b1b2b5267b2ef38aeeb4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
